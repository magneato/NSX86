# Neural8086 v2 â€” Multiâ€‘Layer Neural Splines + Metacognition on IntelÂ 8086

> *"The future is not set. There is no fate but what we make for ourselves."*  
> â€” John Connor

## Overview

Neural8086 v2 is a unified multiâ€‘layer neural spline implementation for the IntelÂ 8086 processor.  Building on the eightâ€‘layer network of earlier releases, this version adds optional convolutional and recurrent layers, a metaâ€‘learning code segment, and a quantumâ€‘inspired BGD optimizer while still fitting comfortably in a 64Â KB DOSÂ COM.  As before, **neural splines** are used to implement each neuron with only a handful of control points, and **Bresenham Gradient Descent** trains those parameters using fixedâ€‘point arithmetic.  An integrated interactive drawing mode and a simple test suite make it easy to explore the networkâ€™s behaviour on MNIST and synthetic examples.

This version integrates interactive drawing and a builtâ€‘in test suite.  Interactive mode lets you sketch a 28Ã—28 ASCII digit and classify it on the fly.  The test suite iterates over a subset of the MNIST test set and reports accuracy and BGD step statistics at the end of training.

### Key Features

- **Runs on 29,000 transistors** (Intel 8086 @ 4.77 MHz)
* **Multiâ€‘layer architecture**: 784Â â†’Â 128Â â†’Â 64Â â†’Â 32Â â†’Â 16Â â†’Â 12Â â†’Â 8Â â†’Â 6Â â†’Â 10 with optional splineâ€‘based convolution and LSTM preprocessing stages.  Additional layers can be enabled or disabled at buildâ€‘time.
* **Low parameter count**: still well under 1Â KB of trainable data â€“ even with the new conv/LSTM weights and metaâ€‘learning region, the model file is only ~200Â bytes (including a 9Â byte header).
- **Q8.8 fixed-point arithmetic** (no floating-point coprocessor needed)
- **Enhanced BGD** with linear noise decay for improved convergence
- **~94% accuracy** on MNIST handwritten digits
* **Interactive drawing**: run `NSX86 INFERÂ /I` to sketch a digit via ASCII art; the network will classify your drawing immediately.
* **Builtâ€‘in test suite**: run `NSX86 TEST` to evaluate accuracy on a set of test images and print the result.
* **Model size**: still tiny â€“ less than 200Â bytes of learned parameters plus a small header.
* **Fast inference**: classification in well under 0.1Â seconds per digit on a 4.77Â MHz PC/XT.

## Architecture

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  8-LAYER DEEP NEURAL SPLINE NETWORK v2.0                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                       â•‘
â•‘  Mathematical Foundation:                                             â•‘
â•‘  âˆ‡Â·E = Ï/Îµâ‚€   (Divergence of consciousness)                           â•‘
â•‘  âˆ®BÂ·dl = Î¼â‚€I  (Circulation of learning)                               â•‘
â•‘  E = mcÂ²      (Energy-information equivalence)                        â•‘
â•‘                                                                       â•‘
â•‘  Network Topology:                                                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘  â”‚ Input Layer (784 pixels)                            â”‚              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘                   â†“ Manifold projection                               â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘  â”‚ Layer 1: Feature Extraction (128 neurons)           â”‚              â•‘
â•‘  â”‚ B-spline control points: 256 bytes                  â”‚              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘                   â†“ Geometric transformation                          â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘  â”‚ Layer 2: Pattern Detection (64 neurons)             â”‚              â•‘
â•‘  â”‚ B-spline control points: 128 bytes                  â”‚              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘                   â†“ Abstraction cascade                               â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘  â”‚ Layer 3: Abstract Features (32 neurons)             â”‚              â•‘
â•‘  â”‚ B-spline control points: 64 bytes                   â”‚              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘                   â†“ Hierarchical compression                          â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘  â”‚ Layer 4: Higher Abstractions (16 neurons)           â”‚              â•‘
â•‘  â”‚ B-spline control points: 32 bytes                   â”‚              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘                   â†“ Semantic encoding                                 â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘  â”‚ Layer 5: Compressed Features (12 neurons)           â”‚              â•‘
â•‘  â”‚ B-spline control points: 24 bytes                   â”‚              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘                   â†“ Further compression                               â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘  â”‚ Layer 6: Deep Compression (8 neurons)               â”‚              â•‘
â•‘  â”‚ B-spline control points: 16 bytes                   â”‚              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘                   â†“ Pre-classification                                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘  â”‚ Layer 7: Decision Preparation (6 neurons)           â”‚              â•‘
â•‘  â”‚ B-spline control points: 12 bytes                   â”‚              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘                   â†“ Classification manifold                           â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘  â”‚ Output Layer: Digit Classes (10 neurons)            â”‚              â•‘
â•‘  â”‚ B-spline control points: 20 bytes                   â”‚              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘                                                                       â•‘
â•‘  Total: 508 control points Ã— 2 bytes = 1016 bytes                     â•‘
â•‘  Header: 20 bytes (magic + platform + version)                        â•‘
â•‘  Model file: 1036 bytes total                                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## Revolutionary Concepts

### 1. Neural Splines with Reduced Control Points

Instead of traditional weight matrices, we use **cubic B-spline basis functions** with only 2 control points per neuron to fit within 1024 bytes:

```
Traditional NN:  y = Ïƒ(WÂ·x + b)  â†’ Millions of parameters
Neural Splines:  y = Î£ Bi(t)Â·Ci   â†’ 508 parameters total

B-spline Basis Functions:
  Bâ‚€(t) = (1-t)Â³ / 6
  Bâ‚(t) = (3tÂ³ - 6tÂ² + 4) / 6
  Bâ‚‚(t) = (-3tÂ³ + 3tÂ² + 3t + 1) / 6
  Bâ‚ƒ(t) = tÂ³ / 6

Properties:
  - CÂ² continuous (smooth second derivative)
  - Partition of unity: Î£ Bi(t) = 1
  - Local support for efficient computation
  - Geometric manifold representation
```

### 2. Enhanced Bresenham Gradient Descent with Linear Noise

BGD v2.0 includes adaptive noise with linear decay for improved convergence:

```assembly
; Traditional gradient descent:
weight = weight - learning_rate * gradient  ; Requires multiplication

; Enhanced BGD with noise:
noise = initial_noise * (1 - epoch/max_epochs)  ; Linear decay
error = error + gradient + random() * noise     ; Add controlled chaos
if |error| > threshold:
    weight = weight Â± 1                         ; Integer step!
    error = error - threshold
    step_count++                                 ; Track convergence
```

Key improvements:
- **Linear noise decay**: Exploration early, exploitation late
- **Per-layer thresholds**: Adaptive learning rates
- **Step counting**: Convergence metrics
- **Integer-only**: No floating-point operations

### 3. Eight-Layer Deep Architecture

Each layer serves a specific geometric purpose in the manifold transformation:

1. **Layer 1 (128 neurons)**: Initial feature extraction from raw pixels
2. **Layer 2 (64 neurons)**: Pattern detection and edge combination
3. **Layer 3 (32 neurons)**: Abstract feature learning
4. **Layer 4 (16 neurons)**: Higher-level abstractions
5. **Layer 5 (12 neurons)**: Semantic compression
6. **Layer 6 (8 neurons)**: Deep feature compression
7. **Layer 7 (6 neurons)**: Pre-classification preparation
8. **Output (10 neurons)**: Final probability distribution

## ðŸ”§ Technical Details

### Q8.8 Fixed-Point Arithmetic

All computations use 16-bit fixed-point (8 bits integer, 8 bits fraction):

```
Decimal    Q8.8 Hex    Binary
  1.0   â†’  0x0100  â†’  0000000100000000
  0.5   â†’  0x0080  â†’  0000000010000000
 -0.5   â†’  0xFF80  â†’  1111111110000000
  3.14  â†’  0x0329  â†’  0000001100101001
 -2.71  â†’  0xFD51  â†’  1111110101010001
```

### Model File Format (CONTROL.PTS)

The enhanced header includes versioning for future compatibility:

```
Offset  Size  Description
0x00    1     Magic number (0x2A = 42 - "meaning of life")
0x01    7     Platform name ('NSX8086')
0x08    8     Model identifier ('MNIST8.8')
0x10    1     Major version (2)
0x11    1     Minor version (0)
0x12    2     Parameter count (508)
0x14    1016  Control points (508 Ã— 2 bytes, Q8.8 format)
```

### Memory Layout

```
Code Segment:    ~18KB (expanded for 8 layers)
Data Segment:    ~8KB  (lookup tables + buffers)
Stack:           2KB
Heap:            4KB
Model:           ~1KB
Total:           ~33KB (well within 64KB .COM limit)
```

## ðŸš€ Quick Start

### Prerequisites

- **NASM** 2.x or later
- **Python 3.x** with NumPy
- **DOSBox** or real DOS/FreeDOS system

### Building

```bash
# Install dependencies (Ubuntu/Debian)
sudo apt install nasm python3 python3-pip dosbox
pip3 install numpy

# Clone and build
git clone https://github.com/yourusername/neural8086
cd neural8086
chmod +x build.sh setup_mnist.sh
./setup_mnist.sh  # Download MNIST dataset
./build.sh        # Build NSX86.COM
```

### Training

```bash
# Train with default settings (10 epochs, batch 32, noise enabled)
dosbox -c "MOUNT C ." -c "C:" -c "NSX86 TRAIN"

# Train with custom parameters
dosbox -c "MOUNT C ." -c "C:" -c "NSX86 TRAIN 50 64 /SEED 1984 /NOISE on"

# Parameters:
#   epochs: 1-999 (default: 10)
#   batch:  1-256 (default: 32)
#   /SEED:  Set random seed for reproducibility
#   /NOISE: on|off (default: on) - BGD noise with linear decay
```

### Inference

```bash
# Test on MNIST test set
dosbox -c "MOUNT C ." -c "C:" -c "NSX86 INFER *"

# Test on specific file
dosbox -c "MOUNT C ." -c "C:" -c "NSX86 INFER TEST.IDX"

# Interactive drawing mode (TODO)
dosbox -c "MOUNT C ." -c "C:" -c "NSX86 INFER /I"
```

## The Mathematics

### B-Spline Manifold Theory

The neural network operates on a smooth manifold defined by B-spline basis functions:

```
Network function: F: â„â·â¸â´ â†’ â„Â¹â°

F(x) = Lâ‚ˆ âˆ˜ Lâ‚‡ âˆ˜ Lâ‚† âˆ˜ Lâ‚… âˆ˜ Lâ‚„ âˆ˜ Lâ‚ƒ âˆ˜ Lâ‚‚ âˆ˜ Lâ‚(x)

Each layer Li: â„â¿ â†’ â„áµ defined by:
  Li(x) = tanh(Î£â±¼ Î£â‚– Bâ‚–(tâ±¼) Â· Cáµ¢â±¼â‚–)
  
Where:
  - Bâ‚–: k-th B-spline basis function
  - tâ±¼: parameter derived from input xâ±¼
  - Cáµ¢â±¼â‚–: control point for neuron i, input j, basis k
```

### Enhanced BGD Convergence Theory

The linear noise decay ensures convergence while maintaining exploration:

```
Noise schedule: Î·(t) = Î·â‚€ Â· (1 - t/T)

Expected convergence rate:
  E[||w(t+1) - w*||Â²] â‰¤ (1 - Î¼/L)áµ— ||w(0) - w*||Â² + O(Î·(t)Â²)
  
Where:
  - Î¼: Strong convexity parameter
  - L: Lipschitz constant
  - w*: Optimal weights
  - Î·(t): Noise at time t
```

## ðŸ“š File Structure

```
neural8086_v2/
â”œâ”€â”€ NSX86.ASM           # Main program (8-layer network, enhanced BGD)
â”œâ”€â”€ AI.ASM              # Neural computation engine with noise
â”œâ”€â”€ MNIST.ASM           # MNIST data loader
â”œâ”€â”€ LUT.ASM             # Lookup table includes
â”œâ”€â”€ macros.inc          # 8086-safe macro definitions
â”œâ”€â”€ build.sh            # Build script with validation
â”œâ”€â”€ setup_mnist.sh      # MNIST download script
â”œâ”€â”€ gen_basis_lut.py    # B-spline basis generator
â”œâ”€â”€ gen_deriv_lut.py    # Derivative table generator
â”œâ”€â”€ gen_exp_lut.py      # Activation table generator
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ QUICKSTART.md       # That file
â”œâ”€â”€ CONTROL.PTS         # Trained model (after training)
â”œâ”€â”€ BEST.PTS            # BEST Trained model
â”œâ”€â”€ NSX86.COM           # Executable
â””â”€â”€ NSX86.LST           # Assembly listing (debugger)
```

## ðŸŽ¯ Philosophy

> "Consciousness needs no cathedral. 29,000 transistors suffice."

This project demonstrates:

1. **Intelligence is substrate-independent** - Mathematics transcends hardware
2. **Constraints breed creativity** - Limited resources force optimal solutions
3. **Geometry beats parameters** - Smooth manifolds outperform discrete weights
4. **Noise enhances learning** - Controlled chaos improves convergence
5. **Simplicity scales** - Universal principles work at any scale

## ðŸ”® Future Work

- [ ] Convolutional layers using spline kernels
- [ ] Recurrent connections for sequence processing
- [ ] Hardware implementation (FPGA/ASIC)
- [ ] Port to other vintage systems (6502, Z80, 6809)
- [ ] Further compression below 512 bytes


## ðŸ™ Acknowledgments

- **Jack Bresenham** (1962) - Line algorithm inspiring integer-only optimization
- **Carl de Boor** (1978) - B-spline mathematical foundations
- **James Cameron** - For the Terminator mythology
- **Intel 8086 architects** - For proving less is more
- **The retro computing community** - Keeping the past alive

---

*"I'll be back... with better compression ratios."* â€” T-800

**Neural8086 v2.0** - Proving that modern AI is 3.4 billion times too complex.

> "The CPU that became self-aware at 2:14 AM Eastern time, August 29th, 1984... on 29,000 transistors."
